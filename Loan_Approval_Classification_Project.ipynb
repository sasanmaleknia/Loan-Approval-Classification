{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bedb6ae",
   "metadata": {},
   "source": [
    "## Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8942f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical and Data Manipulation \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn Pipeline and Preprocessing \n",
    "from sklearn.pipeline import Pipeline            \n",
    "from sklearn.compose import ColumnTransformer         \n",
    "\n",
    "#Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report, \n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,            # To split data into training and test sets\n",
    "    GridSearchCV,\n",
    "    learning_curve\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression      \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Data Preprocessing \n",
    "from sklearn.impute import SimpleImputer                 # To fill in missing values\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,              # To standardize features (zero mean, unit variance)\n",
    "    OneHotEncoder                # To encode categorical variables as binary vectors\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignoring warning messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6938a5ad",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aca454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"loan_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795cb65",
   "metadata": {},
   "source": [
    "## Dataset Attributes:\n",
    "\n",
    "The dataset contains the following attributes:\n",
    "\n",
    "Loan_ID – Unique identifier for each loan application\n",
    "\n",
    "Gender – Gender of the applicant (Male/Female)\n",
    "\n",
    "Married – Marital status of the applicant (Yes/No)\n",
    "\n",
    "Dependents – Number of dependents (e.g., 0, 1, 2, 3+)\n",
    "\n",
    "Education – Education level (Graduate/Not Graduate)\n",
    "\n",
    "Self_Employed – Employment status (Yes/No)\n",
    "\n",
    "ApplicantIncome – Monthly income of the applicant (numeric)\n",
    "\n",
    "CoapplicantIncome – Monthly income of the co-applicant (numeric)\n",
    "\n",
    "LoanAmount – Loan amount requested (in thousands)\n",
    "\n",
    "Loan_Amount_Term – Duration of the loan in days\n",
    "\n",
    "Credit_History – Credit history (1 = Good, 0 = Bad)\n",
    "\n",
    "Property_Area – Area type where the property is located (Urban, Semiurban, Rural)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aec861",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c81a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Looking at the dataset:\")\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bfc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"dataset shape: \\n:{df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf19ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target distribution:\\n\", y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"dataset information:\\n {df.info()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Missing values:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Description of Numerical Features:\")\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating the Correlation Heatmap:\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr = df.corr(numeric_only=True)\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True)\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    \"person_gender\",\n",
    "    \"person_education\",\n",
    "    \"person_home_ownership\",\n",
    "    \"loan_intent\",\n",
    "    \"previous_loan_defaults_on_file\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    sns.countplot(data=df, x=feature, hue=\"loan_status\", ax=axes[i])\n",
    "    axes[i].set_title(f\"Loan Status by {feature}\")\n",
    "    axes[i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "if len(categorical_features) % 2 != 0:\n",
    "    fig.delaxes(axes[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e9754",
   "metadata": {},
   "source": [
    "# Pre_processing and Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    var for var in df.columns if df[var].dtypes == \"object\"\n",
    "]  # Identify categorical features\n",
    "numerical_features = [\n",
    "    var for var in df.columns if df[var].dtypes != \"object\"\n",
    "]  # Identify numerical features\n",
    "numerical_features.remove(\"loan_status\")\n",
    "\n",
    "print(f\"Categorical columns:\\n {categorical_features}\")\n",
    "print(f\"Numerical columns:\\n {numerical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd27f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"person_gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical features: Imputation and Scaling\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"imputer\",\n",
    "            SimpleImputer(strategy=\"mean\"),\n",
    "        ),  # Replace missing values with column mean\n",
    "        (\"scaler\", StandardScaler()),  # Standardize features (zero mean, unit variance)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# categorical features: Imputation and One-Hot Encoding\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"imputer\",\n",
    "            SimpleImputer(strategy=\"most_frequent\"),\n",
    "        ),  # Replace missing values with most frequent category\n",
    "        (\n",
    "            \"onehot\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "        ),  # Convert categories to one-hot vectors, ignore unseen categories\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3933cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ColumnTransformer to apply different transformations to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_transformer, numerical_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",  # Drop columns not specified\n",
    "    verbose_feature_names_out=True,  # To get clean feature names out of OneHotEncoder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01db48c",
   "metadata": {},
   "source": [
    "## Spliting the Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62591958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (80/20), stratified by the target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=616\n",
    ")\n",
    "\n",
    "# Display the shapes of the splits\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}\")\n",
    "print(f\"y_test shape:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1efe7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", LogisticRegression()),  # Placeholder\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        \"classifier\": [LogisticRegression(solver=\"saga\", max_iter=1000)],\n",
    "        \"classifier__C\": [0.01, 0.1, 1, 10], # Learning Rate\n",
    "        \"classifier__penalty\": [\"l1\", \"l2\"], # Lasso and Ridge Regularizers \n",
    "    },\n",
    "    {\n",
    "        \"classifier\": [RandomForestClassifier()],\n",
    "        \"classifier__n_estimators\": [50, 100],\n",
    "        \"classifier__max_depth\": [5, 10],          # (reduce overfitting)\n",
    "        \"classifier__max_features\": [\"sqrt\"],\n",
    "        \"classifier__class_weight\": [\"balanced\"],\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search with both LogisticRegression and RandomForestClassifier\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                       # 5-fold cross-validation\n",
    "    scoring= \"accuracy\",         # metric\n",
    "    n_jobs=-1               \n",
    ")\n",
    "# Fit the grid search\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fefa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\\n\", grid.best_params_)\n",
    "print(\"Best model:\\n\", grid.best_estimator_)\n",
    "print(\"Best cross-validation score:\\n\", grid.best_score_)\n",
    "\n",
    "# Predict using the best model\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6265e5",
   "metadata": {},
   "source": [
    "## Final Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d2d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "#  Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9e7e63",
   "metadata": {},
   "source": [
    "## learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    estimator=best_model, X=X_train, y=y_train,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), cv=5, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_sizes, train_mean, label='Training Accuracy', marker='o')\n",
    "plt.plot(train_sizes, val_mean, label='Validation Accuracy', marker='o')\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13280d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
